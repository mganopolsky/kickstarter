---
title: "PH125.9x Data Science Capstone - Kaggle Kickstarter Dataset Analysis and Predictions"
author: "Marina Ganopolsky"
date: "12/3/2020"
output: 
  pdf_document: 
    fig_height: 5
    fig_width: 6
    keep_tex: yes
    latex_engine: xelatex
    toc: yes
    toc_depth: 5
mainfont: Montserrat
toc: true
header-includes: 
- \usepackage{amsmath}
- \usepackage{fontspec}
- \setmainfont{Montserrat}
number_sections: true
graphics: yes
toc_depth: 5
df_print: kable
fontsize: 12pt
editor_options: 
  chunk_output_type: inline
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
```


```{r, include=FALSE, cache=FALSE, echo=FALSE}
knitr::read_chunk('kickstarter.R')
```

```{r include=FALSE, echo=FALSE}
threshold_value <- 0.5
```

\pagebreak

# Overview

This is Marina Ganopolsky's implementation of the **Choose Your Own Project** of the **Harvard: PH125.9x Data Science, Summer 2020: Capstone** course.\newline

**The objectives of this project are:**


1. To find a dataset on which to conduct Exploratory Data Analysis (EDA), Feature Engineering, and on which to perform **machine learning tasks**.
2. To use as least 2 methods beyond linear regression during the project.
3. To achieve an accuracy of **> 50%**, and validate this with the **test data set**, as described below.
3. To summarize the findings, draw conclusions, and provide further recommendations, if needed.

The dataset I have chosen is the [Kickstarter Dataset from Kaggle](https://www.kaggle.com/kemical/kickstarter-projects?select=ks-projects-201801.csv). This dataset represents Kickstarter projects between the years of **2009 and 2018**, as well as their respective success and failure information. Information about the country of origin of the project, the category, the goal amount, currency, etc is also present. 
A quick summary of the data wrangling modifications will be provided, as well as some visual representations of patterns present in the data, in the **Method and Analysis** section. This will inform the reader about the various trends present in the dataset.

An exploratory analysis will be performed to generate the predicted succeses levels of the Kickstarter Project, with various models, to be specified; a best-performing algorithm will be chosen, and the results of the calculations will be analyzed and explained; finally a conclusion will be provided.


Several models will be developed, ranging from the most naive to the most complex. These will be tested and compared using the resulting RMSE in order to assess their quality. In contrast to the MovieLens project there is no ideal evaluation criteria for this project. However, since we have several models available, and we will pick the model with the highest accuracy as the chosen model to produce the accuracy of the model.

The data is broken into a **training dataset** (80%), and a **testing dataset**(20%). After evaluating every available algorithm I have come up with, the best-resulting model will be used on the **test dataset** to evaluate the quality of the predictions of the movie ratings. 

This project can be found on **GitHub** [here](https://github.com/mganopolsky/kickstarter) .   

## Dataset
```{r, libraries, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```

```{r, load_data, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```

The **Kickstarter** dataset is automatically downloaded with the code provided; The data sets can be additionally downloaded here:

* https://raw.githubusercontent.com/mganopolsky/kickstarter/master/data/ks-projects-201801.csv

As specified by the project description (and the provided code) the data is split into a **test set (20%)** and the **training set(80%)**, after some feature engineering is applied to it. The calculations and algorithm verification are done on the training set, and each algorithm is tested on the test set.

As a first step, in order to get some basic information about the data we're working with, we examine the **```kickstarter```** dataset. The original dataset contains 15 variables :
```{r, glimpse_data, echo=TRUE, include=TRUE, warning=FALSE}
```

A fair amount of data manipulation has been done to the datasets created with the provided code. The changes include:
```{r, massage_data, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```
* **New Field** : Adding a time interval in days, as the difference between the launch date and the deadline date : **time_int**
* **New Field** : Extracting the month of the year from the launch date, as saving it as a factor in **launched_month**
* **New Field** : Extracting the year from the launch date, as saving it as a factor in **launched_year**
* **New Field** : Extracting the day of the week from the launch date, as saving it as a factor in **launched_day_of_week**
* **New Field** : Added a field as a rounded numeric representing the pledge:goal ratio in USD **pledged_ratio**
* **New Field** : Added a field as a rounded numeric representing the average pledge per backer in USD **avg_backer_pldg**
* Changed the **launched** field into a date (as opposed to a date/time field)
* **Removed Field** : Removed the  **`usd pledged`** field, as there was the usd_pledged_real represented what was actually needed.

* After the data wrangling and data visualization exercises (see below) are complete, the dataset is split up into :
  + **A training dataset** - the data the algorithms will be trained on, **80%** of the remaining data.
  + **A testing dataset** - the data the algorithms will be tested on, **20%** of the remaining data.

Per row, this is a representation of a single Kickstarter campaign. \newline
**As a first glance, here's a snapshot of dataset:**

```{r, summary_ds, echo=FALSE}
```

**The first few lines of the dataset look like this:**

```{r,  echo = FALSE, tidy=TRUE}
head(ds) %>%  print.data.frame()
```

# Methods and Analysis

## Data Insights, Cleaning & Visualizations

### Initial Insights

Glancing over the information most things seem fine at a first review; one thing that jumps out is the minimum value of the **launched** field, which is 1/1/1970. This is probably an error in the data - let's examine it:

```{r, launched_ds, echo=TRUE, include=TRUE, warning=FALSE}
```

Viewing a list of the furthest ordered launch dates, we see that 1/1/1970 is a strange outlier. We dig further by analyzing the entries with this information:
```{r, launched_1970, echo=TRUE, include=TRUE, warning=FALSE}
```

Based on the names, most of these campaigns have been cancelled. It's worth noting that in the predictive modeling section, our models will exclude projects that do not fall into the "successful" or "failed". Therefor, we will remove these entires from the data.

```{r, delete_1970, echo=FALSE, include=FALSE, warning=FALSE}
```

### Pledge Information

What kind of information can be gleaned from failed projects? We examine projects where the pledge ratio is less then 1 (meaning that they did not meet their fundraise goal.)
```{r, pledge_less_1, echo=TRUE, include=TRUE, warning=FALSE}
```

It appears that some projects are succesful despite not having met their funding goal!
```{r, pledge_less_2, echo=TRUE, include=TRUE, warning=FALSE}
```

The project raised a significant sum though - 85% of what it neeeded; and it went on to be successful. However, it's the only one of its kind.
This of course, begs the question of what happens 

Viewing the pledge ratios' cumulative distribution, **it is obvious that most of the successful projects are funded around 100%  or slightly higher - with the ratios hovering around 1. However, there are actually a significant amount of projects that are funded at thousands of times their goal value.**:\newline

```{r ECDF, echo=FALSE, warning=FALSE, results='hide'} 
```

### Data Manipulation
Since we will be using models that require fields set as factors, the predictive fields of the dataset will be manipulated to be changed into factors. 
```{r factors3, echo=FALSE, warning=FALSE, results='hide'} 
```
The levels of the factors of these fields can be viewed here:
```{r levels, echo=TRUE, warning=FALSE} 
```
It now seems that the country field has some strange values ("N,0\"") in it; we will remove these records.
```{r filtere_N0, echo=FALSE, warning=FALSE} 
```

### Categories & Funds Pledged
Each projecet has a **main_category** and a **category** section. Which **main_category** do users attempt to fund most often?
```{r freq, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```
In terms of the main categoriex, **Film & video** projects seem to be the **most** wide-spreead, while **Dance** is the **least**.
What does this look like in terms of funds raised?
```{r raised, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```
And what about sub-categories?
```{r raised_by_sub_code, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```

### Project States

```{r proj_by_state, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```

From the data and the bar graph of the project states, we can make these conclusions: 
"
1. The vast majority of the projects fall into the **"failed"** or **"successful"** category. 
2. There are 6 discrete optionss.
3. Most projects **FAIL**.

The information in the dataset includes some facts, but is hardly a full picture of what happens in a project. We don't know about the marketing efforts, time/funds spent on those, or anything else. Therefor, it's hardly a detailed model we're building, but it's a start. As mentioned earlier, we will be focusing exclusively on the projecets that fall into the **"failed"** or **"successful"** categories.

### Pledged vs Goal 
Let's examine the distribution of pledged value and goal value in successful vs failed projects; As is evidenced by the densty plots on the left side, little info can be gleaned here. However, once the visualization is **log-transformed**, the distributions of the successful projects seem to be nearing normal. The pledged values are normally distributed for succesful projects, but not so much for the failed ones; this makes intuituve sense since many failed projects had raised no money at all.
```{r medians_log_distribution, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```
Digging into this further, a box plot of pledge ratios paints a very clear picture of failed vs. successful projects - and this makes sense intuitively, as well. We've already seen that nost successful projets are funded at over 100$ of goal.

```{r failed_quantiled, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

And if we examine the pledge ratios quantiles, this is also obvious. 
Faled:
```{r suc_quantiled, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

### Feature Correlation & Pairs

Correlation of some of the features and overview of their relationship is shown here. The only items correlated very slightly are the time interval and the goal amount in USD. This makes sense intuitively as the planners expect to raise some amount of money in a specific time.

```{r ggpairs, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```
It looks like the only real correlation - .0022 is shown between pledged amounts and the amount of backers. 
This makes sense of course, since the amount of backers will be a small indicator of the amount of funds raised.  

```{r final_ds_selection, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```

## Functions of Note

This project used several different functions to help with the creation of dynamic model formulas, as well as some wrappers to quickly and neatly get accuracy and confusion matrix data. I'm including these here.

```{r GLM_functions, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

## Rating Models
```{r data_split, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
```
Since we are trying to predict the variable **state** and it is **categorical**, as well as are many of the predictors, **linear regression was not a good option for for this project**. Therefor, we attempt the models below.

### 1. Classification Trees

The first model I will attempt will be **classification trees**. The initial attempt used the following parameters:
1. maxdepth = 5 ( the maximum depth of any node of the final tree)
2. minsplit = 200 ( the minimum number of observations that must be observed before splitting the tree)
```{r ct_model, echo=FALSE, include=TRUE, results='show'}
```

The next attempt once again runs the model, but this time with all default parameters, save **cp**, the complexity parameter, which we set to 0. The logic behind this is that we make a detailed tree and prune it AFTER the model has run its course. **The plot of the cp values** is shown below, showing the optimal (lowest) value for the relative error. 

```{r ct_pruned_model, echo=FALSE, include=TRUE, results='show'}
```

The code then selects this minimal value, **`r cp`**, and prunes the tree with that complexity paramter. The accuracy of the resulting model is **`r ct_pruned_accuracy`**. As I continue with the project, I will display results of the accuracy evaluations of every model, and print them out as shown below - so that the data can be reviewed as the project progresses.\newline

```{r  echo=FALSE}
model_results %>% knitr::kable()
```

Next, still attempting to optimize classification trees, I will attempt to run the model with ALL default parameters set, and then again post-prune the resulting tree by finding the minimum **cp** parameter.  

```{r ct_model_2, echo=FALSE, include=TRUE, results='show'}
```
This time, the code selects a minimal cp value of  **`r cp`** to prune the tree. The accuracy of the resulting model is **`r ct_pruned_accuracy2`**. \newline
```{r  echo=FALSE}
model_results %>% knitr::kable()
```

###  2. Random Forest

```{r ct_model_2, echo=FALSE, include=FALSE}
```
When first attempting to run the **random forest** algorithm on the data, it became obvious that the general implementation wouldn't be plausable here due to some of the factors (the categories in particular) having more then 32 levels. (This is a limitation of the random forest algorithm implementation in R.) Therefor, the data had to be converted into a matrix with dummy variables. A sample of 70,000 was taken to evaluate the training set; the processing time for this was very long - and in the future implementation of the model I will attempt to run random forest on a larger subset of data. The final random forest model accuracy with **200 trees and a 70,000 samples** was **`r rf_accuracy`**. \newline

```{r  echo=FALSE}
model_results %>% knitr::kable()
```
### 3. Naive Bayes
```{r nb_default, echo=FALSE, include=FALSE}
```
The next attemped model is the Naive Bayes Model. In the initial attempt, I used the same training/testing dataframes as in the classification trees attempts described above. This produced an accuracy of **`r nb_accuracy`**, which is so far the lowest we've seen (and will prove to be the worst). 
```{r  echo=FALSE}
model_results %>% knitr::kable()
```

```{r nb_matrix, echo=FALSE, include=FALSE}
```
However, the algorithm also takes matrices as input, and at the next iteration the model will attempt to evaluate the accuracy on the matrix data (which is simply the dataset with dummay variables.) Here, the accuracy was much higher, at **`r nb_m_accuracy`**. 
```{r  echo=FALSE}
model_results %>% knitr::kable()
```

### 4. Generalized Linear Models (GLM)

The next, and most extensive set of models uses **Generalized Linear Regression** - a variation of linear regression that allows for predictions of categorical variables. Since here we are trying to predict the 

#### 4.1 Movie Effect Regularized Model




# Results

After creating a number of predictive algorithms, the best model as per the project requirements is the one with the least RMSE : **'Regularized Movie, User, Release Year and Genre Effect Model'** (This is model **#7** from the list enclosed below). Now that we've selected this model, it will be applied to the **validation data set** (which has been untouched as of yet) with the $\lambda$ value we calculated with the last function.

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:final_rmse")
archivist::loadFromLocalRepo(hash)
```

## The final RMSE value calculated from the validation set is: **`r final_rmse`**.

`r final_rmse` is well below the required threshold of **`r threshold_value`**.\newline\newline
**We can now review the complete set of RMSE calculations :**

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse_results")
archivist::loadFromLocalRepo(hash)
```

```{r regularized_movie_and_user_and_year_and_genre_validation_results_final_ouput, echo=FALSE}
```

It is apparent from the numbers presented above that without regularization we cannot achieve RMSE numbers less then the threshold value of **`r threshold_value`**.\newline\newline 

I wanted to examine options for improvement of the algorithm and introduced regularization into the models. The simplest regularization model, using only the movie affect with an optimized lambda, was not good enough for our calculations, and actually regressed from our third model (shown above). However, regularization with movie AND user effect beat the previously best-performing model. Adding in the release year in model # 6 increased our performance even further, and finally, using regularization combined with the movie, user, release year and genre effects proved to be the most efficient model of the models tested.\newline\newline
**Applying this final model, #7, to the validation set, gave us a validated RMSE of `r final_rmse`**. 
\pagebreak

# Conclusion

## Report Summary

In this project, I have built a machine learning algorithm to predict movie ratings with MovieLens dataset, and estimate their accuracy with RMSE. The **regularized model including the effect of user, movie, release year and genre** is characterized by the lowest RMSE value of all I have examined, and is hence the optimal model to use for the present project - out of the models presented. 

The final rating model uses  **regularization**, which constrains the total variability of the effect sizes by penalizing large estimates that come from small sample sizes. With a tuning parameter $\lambda$ to generate a rating  $Y_{u,i}$ for movie $i$ by user $u$. These calculations can be summarized with the following formula:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u - b_y -b_g \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2 + \sum_{i} b_y^2 + \sum_{i} b_g^2 \right)$$

where the variables are:

* **$\mu$** is the average rating across all movies
* **$b_i$** is the per-movie movie bias
* **$b_u$** is the per-user movie bias
* **$b_y$** is the age of movie (year) bias
* **$b_g$** is the genre bias

The first term in the equation above is the mean squared error calculation , and the second term is the penalty term that gets larger when all the effects listed above (movie effect, user effect, release year effect, and genre effect) are large. Re-working the equation using calculus we can show that we need a $\lambda$ value that will minimize the equation that adds the penalty for all of the effects mentioned. 

The **RMSE** arrived at with this model on the **validation dataset** is **`r final_rmse`** - which is sufficiently accurate, as it is lower than the initial evaluation criteria **`r threshold_value`** given by the goal of the present project.\newline\newline

**As mentioned above, due to the way the function ```createDataPartition``` works, movies with 3 reviews or less have been excluded from the dataset in order to prevent NA errors in the RMSE calculations.** 

\pagebreak

## Other Options

### Using Various Regression Functions.
During this process I have also attempted to run the predictions using the ```train()``` R function,   with methods that included Linear Regression ("lm"), LASSO regression ("glmnet"), and Generalized Linear Regression ("glm"). When conducted on a small part of the data set, these functions produced an inferior RMSE result, not in line with the threshold required. More importantly, all 3 of these methods crashed the system when attempted to be run on the full dataset - showing that these are not realistic tools to be used for datasets of this size (in the millions).

### Possible Improvements

I think improvements on the RMSE could be achieved by evaluating the project with other means. Different machine learning models could also improve the results further, but hardware limitations (memory and processing power) may be a constraint.

The possible evaluation models we can also attempt to get even better results would include:

* Matrix factorization / Single Value Decomposition (SVD) / Principal Component Analysis (PCA)
* Gradient Descent 
* SGD Code Walk


\pagebreak

# Appendix - Environment

```{r}
print("Operating System:")
version
```