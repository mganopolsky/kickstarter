---
title: "PH125.9x Data Science Capstone - Kaggle Kickstarter Dataset Analysis and Predictions"
author: "Marina Ganopolsky"
date: "12/3/2020"
output: 
  pdf_document: 
    fig_height: 5
    fig_width: 6
    keep_tex: yes
    latex_engine: xelatex
    toc: yes
    toc_depth: 5
mainfont: Montserrat
toc: true
header-includes: 
- \usepackage{amsmath}
- \usepackage{fontspec}
- \setmainfont{Montserrat}
number_sections: true
graphics: yes
toc_depth: 5
df_print: kable
fontsize: 12pt
editor_options: 
  chunk_output_type: inline
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
```


```{r, include=FALSE, cache=FALSE, echo=FALSE}
knitr::read_chunk('kickstarter.R')
```

```{r include=FALSE, echo=FALSE}
threshold_value <- 0.5
```

\pagebreak

# Overview

This is Marina Ganopolsky's implementation of the **Choose Your Own Project** of the **Harvard: PH125.9x Data Science, Summer 2020: Capstone** course.\newline

**The objectives of this project are:**


1. To find a dataset on which to conduct Exploratory Data Analysis (EDA), Feature Engineering, and on which to perform **machine learning tasks**.
2. To use as least 2 methods beyond linear regression during the project.
3. To achieve an accuracy of **> 50%**, and validate this with the **test data set**, as described below.
3. To summarize the findings, draw conclusions, and provide further recommendations, if needed.

The dataset I have chosen is the [Kickstarter Dataset from Kaggle](https://www.kaggle.com/kemical/kickstarter-projects?select=ks-projects-201801.csv). This dataset represents Kickstarter projects between the years of **2009 and 2018**, as well as their respective success and failure information. Information about the country of origin of the project, the category, the goal amount, currency, etc is also present. 
A quick summary of the data wrangling modifications will be provided, as well as some visual representations of patterns present in the data, in the **Method and Analysis** section. This will inform the reader about the various trends present in the dataset.

An exploratory analysis will be performed to generate the predicted succeses levels of the Kickstarter Project, with various models, to be specified; a best-performing algorithm will be chosen, and the results of the calculations will be analyzed and explained; finally a conclusion will be provided.


Several models will be developed, ranging from the most naive to the most complex. These will be tested and compared using the resulting RMSE in order to assess their quality. In contrast to the MovieLens project there is no ideal evaluation criteria for this project. However, since we have several models available, and we will pick the model with the highest accuracy as the chosen model to produce the accuracy of the model.

The data is broken into a **training dataset** (80%), and a **testing dataset**(20%). After evaluating every available algorithm I have come up with, the best-resulting model will be used on the **test dataset** to evaluate the quality of the predictions of the movie ratings. 

This project can be found on **GitHub** [here](https://github.com/mganopolsky/kickstarter) .   

## Dataset
```{r, libraries, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```

```{r, load_data, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```

The **Kickstarter** dataset is automatically downloaded with the code provided; The data sets can be additionally downloaded here:

* https://raw.githubusercontent.com/mganopolsky/kickstarter/master/data/ks-projects-201801.csv

As specified by the project description (and the provided code) the data is split into a **test set (20%)** and the **training set(80%)**, after some feature engineering is applied to it. The calculations and algorithm verification are done on the training set, and each algorithm is tested on the test set.

As a first step, in order to get some basic information about the data we're working with, we examine the **```kickstarter```** dataset. The original dataset contains 15 variables :
```{r, glimpse_data, echo=TRUE, include=TRUE, warning=FALSE}
```

A fair amount of data manipulation has been done to the datasets created with the provided code. The changes include:
```{r, massage_data, echo=FALSE, include=FALSE, results='hide', warning=FALSE}
```
* **New Field** : Adding a time interval in days, as the difference between the launch date and the deadline date : **time_int**
* **New Field** : Extracting the month of the year from the launch date, as saving it as a factor in **launched_month**
* **New Field** : Extracting the year from the launch date, as saving it as a factor in **launched_year**
* **New Field** : Extracting the day of the week from the launch date, as saving it as a factor in **launched_day_of_week**
* **New Field** : Added a field as a rounded numeric representing the pledge:goal ratio in USD **pledged_ratio**
* **New Field** : Added a field as a rounded numeric representing the average pledge per backer in USD **avg_backer_pldg**
* Changed the **launched** field into a date (as opposed to a date/time field)
* **Removed Field** : Removed the  **`usd pledged`** field, as there was the usd_pledged_real represented what was actually needed.

* After the data wrangling and data visualization exercises (see below) are complete, the dataset is split up into :
  + **A training dataset** - the data the algorithms will be trained on, **80%** of the remaining data.
  + **A testing dataset** - the data the algorithms will be tested on, **20%** of the remaining data.

Per row, this is a representation of a single Kickstarter campaign. \newline
**As a first glance, here's a snapshot of dataset:**

```{r, summary_ds, echo=FALSE}
```

**The first few lines of the dataset look like this:**

```{r,  echo = FALSE, tidy=TRUE}
head(ds) %>%  print.data.frame()
```

# Methods and Analysis

## Data Insights, Cleaning & Visualizations

### Initial Insights

Glancing over the information most things seem fine at a first review; one thing that jumps out is the minimum value of the **launched** field, which is 1/1/1970. This is probably an error in the data - let's examine it:

```{r, launched_ds, echo=TRUE, include=TRUE, warning=FALSE}
```

Viewing a list of the furthest ordered launch dates, we see that 1/1/1970 is a strange outlier. We dig further by analyzing the entries with this information:
```{r, launched_1970, echo=TRUE, include=TRUE, warning=FALSE}
```

Based on the names, most of these campaigns have been cancelled. It's worth noting that in the predictive modeling section, our models will exclude projects that do not fall into the "successful" or "failed". Therefor, we will remove these entires from the data.

```{r, delete_1970, echo=FALSE, include=FALSE, warning=FALSE}
```

### Pledge Information

What kind of information can be gleaned from failed projects? We examine projects where the pledge ratio is less then 1 (meaning that they did not meet their fundraise goal.)
```{r, pledge_less_1, echo=TRUE, include=TRUE, warning=FALSE}
```

It appears that some projects are succesful despite not having met their funding goal!
```{r, pledge_less_2, echo=TRUE, include=TRUE, warning=FALSE}
```

The project raised a significant sum though - 85% of what it neeeded; and it went on to be successful. However, it's the only one of its kind.
This of course, begs the question of what happens 

Viewing the pledge ratios' cumulative distribution, **it is obvious that most of the successful projects are funded around 100%  or slightly higher - with the ratios hovering around 1. However, there are actually a significant amount of projects that are funded at thousands of times their goal value.**:\newline

```{r ECDF, echo=FALSE, warning=FALSE, results='hide'} 
```

### Data Manipulation
Since we will be using models that require fields set as factors, the predictive fields of the dataset will be manipulated to be changed into factors. 
```{r factors3, echo=FALSE, warning=FALSE, results='hide'} 
```
The levels of the factors of these fields can be viewed here:
```{r levels, echo=TRUE, warning=FALSE} 
```
It now seems that the country field has some strange values ("N,0\"") in it; we will remove these records.
```{r filtere_N0, echo=FALSE, warning=FALSE} 
```

### Categories & Funds Pledged
Each projecet has a **main_category** and a **category** section. Which **main_category** do users attempt to fund most often?
```{r freq, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```
In terms of the main categoriex, **Film & video** projects seem to be the **most** wide-spreead, while **Dance** is the **least**.
What does this look like in terms of funds raised?
```{r raised, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```
And what about sub-categories?
```{r raised_by_sub_code, echo=FALSE, warning=FALSE, include=TRUE, message=FALSE} 
```

### Project States

```{r proj_by_state, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```

From the data and the bar graph of the project states, we can make these conclusions: 
"
1. The vast majority of the projects fall into the **"failed"** or **"successful"** category. 
2. There are 6 discrete optionss.
3. Most projects **FAIL**.

The information in the dataset includes some facts, but is hardly a full picture of what happens in a project. We don't know about the marketing efforts, time/funds spent on those, or anything else. Therefor, it's hardly a detailed model we're building, but it's a start. As mentioned earlier, we will be focusing exclusively on the projecets that fall into the **"failed"** or **"successful"** categories.

### Pledged vs Goal 
Let's examine the distribution of pledged value and goal value in successful vs failed projects; As is evidenced by the densty plots on the left side, little info can be gleaned here. However, once the visualization is **log-transformed**, the distributions of the successful projects seem to be nearing normal. The pledged values are normally distributed for succesful projects, but not so much for the failed ones; this makes intuituve sense since many failed projects had raised no money at all.
```{r medians_log_distribution, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```
Digging into this further, a box plot of pledge ratios paints a very clear picture of failed vs. successful projects - and this makes sense intuitively, as well. We've already seen that nost successful projets are funded at over 100$ of goal.

```{r failed_quantiled, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

And if we examine the pledge ratios quantiles, this is also obvious. 
Faled:
```{r suc_quantiled, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

### Feature Correlation & Pairs

Correlation of some of the features and overview of their relationship is shown here. The only items correlated very slightly are the time interval and the goal amount in USD. This makes sense intuitively as the planners expect to raise some amount of money in a specific time.

```{r ggpairs, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```
It looks like the only real correlation - .0022 is shown between pledged amounts and the amount of backers. 
This makes sense of course, since the amount of backers will be a small indicator of the amount of funds raised.  

```{r final_ds_selection, warning=FALSE, message=FALSE, echo=FALSE, include=TRUE}
```

## Functions of Note

This project used several different functions to help with the creation of dynamic model formulas, as well as some wrappers to quickly and neatly get accuracy and confusion matrix data. I'm including these here.

```{r GLM_functions, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE}
```

## Rating Models
```{r data_split, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
```
Since we are trying to predict the variable **state** and it is **categorical**, as well as are many of the predictors, **linear regression was not a good option for for this project**. Therefor, we attempt the models below.

### 1. Classification Trees

The first model I will attempt will be **classification trees**.
```{r ct_model, echo=FALSE, include=FALSE, results='show'}
```

```{r lambda_plot_names, echo=FALSE, include=FALSE, results='hide'}
```

To start out, we will examine the **mean rating**, and use that as the benchmark for evaluations, creating a model that assumes the same rating for all movies and users with all the differences explained by random variation,  $\varepsilon _{u,i}$.  The formula used for this is:
$$Y_{u,i} = \mu  + \varepsilon_{u,i}$$
$Y_{u,i}$ is the predicted rating per user, per movie and $\mu$ is the estimate that minimizes the root mean squared error — is the average rating of all movies across all users. The code below calculates $\mu$ on the training set of the data, and then uses the RMSE to predict the error of ratings of the test set.

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse_results_output_0")
archivist::loadFromLocalRepo(hash)
```

This value will be used with the RMSE function to test the accuracy with most naive version of the algorithm. The output is below:

```{r  echo=FALSE}
rmse_results_output_0 %>%  knitr::kable()
```


As I continue with the project, I will display results of the RMSE evaluation of every model, and print out the table as I have above - so that the data can be reviewed as the project progresses.\newline
Taking the average of the existing ratings is a very naive predictor. We can see that the RMSE is > 1.0, which means this isn't a great prediction - as expected. In the rest of the project I will examine several better approaches.

###  2. Movie effect model

From the data analysis performed earlier, we've surmised that some movies will be rated higher then others - making the naive RMSE above clearly inferior to a more detailed approach. I will now build a model with accounting for bias per movie,  $b_{i}$ (since every movie will have some inherent bias, and some movies are clearly better (or worse) then others).

$$Y_{u,i} = \mu  + b_{i} + \varepsilon_{u,i}$$
$b_{i}$ is the average of $Y_{u,i}$, minus the overall mean for each movie $i$.
To calculate the movie bias, we transform the formula to :
$$b_{i} =  Y_{u,i} - \mu  - \varepsilon _{u,i}$$

And run the RMSE algorithm on the test set again. The predictions improve significantly when we use this model, as shown below.

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse_results_output_1")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:model_1_rmse")
archivist::loadFromLocalRepo(hash)
```

```{r  echo=FALSE}
rmse_results_output_1 %>% knitr::kable()
```

Adding a single **movie effect** to the model not only significantly reduces the RMSE to **`r model_1_rmse`**, but brings it below 1.0, which is a great first step. However, the number is still not low enough, so we continue on.

### 3. Movie and user effect model

We compute the user bias **$b_{u}$** per user. Some users are cranky, and others are optimistic - so the Effect of the user's crankiness can change the ratings in either direction, positive or negative. The formula for the predicted rating would then be, building on what we have already seen in models 1 and 2 above:

$$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$$

We compute an approximation by computing $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of 
$$b_{u} = Y_{u,i} - \mu - b_{i} - \epsilon_{u,i}$$

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse_results_output_2")
archivist::loadFromLocalRepo(hash)
```

```{r  echo=FALSE}
rmse_results_output_2 %>%  knitr::kable()
```

### 4. Regularized Models

So far we have been computing estimates with certain levels of uncertainty, but we have not been taking into accounts that problems may exist in the data where there is not enough information. There is always a risk of over-fitting the data, and to reduce this risk, we introduce the concept of **regularization**. Regularization will allow us to use a tuning parameter, $\lambda$, that when included in the calculation will minimize the mean squared error. This will reduce the $b_{i}$ and $b_{u}$ in case of small number of ratings. 

Since I am attempting to narrow down a $\lambda$ value quite frequently during this project over various models, I have created a function that will be used to create, calculate, plot, and narrow down the lambda range for each model I attempt to train. 

The function takes in parameters for the start and end of the range of the $\lambda$, an increment to step through the  $\lambda$ values, as well as another function that is passed in to do the calculations that vary between the models. The function then recursively calls itself only **once** to get an even finer-grained value of  $\lambda$, with a smaller, narrower set of lambdas once a rough range has been established in the first run-through. The function returns a minimum lambda for the model in question; This value, in turn, can then be used to calculate the RMSE for the model. The testing set is very large so if we want to evaluate lambdas somewhat efficiently, we first run the data set with broad intervals, and then zoom in on the best performing section - this is the point of the recursive call of the function, named ```find_generic_lambda```. The function will also plot the  $\lambda$ vs. RMSE values for both the wide and narrow iteration of the function, choosing the lowest-available value.

**The function will also plot both sets of attempted $\lambda$'s ** - first, the **wide range** that is set by the first call of the function, and then, the **narrowed range** with much more granular steps. Since this function will be called by every regularized model, we will see the $\lambda$ ranges plotted appropriately.


#### 4.1 Movie Effect Regularized Model

```{r find_generic_lambda, eval=FALSE}
```


The first model to use this function will be the **Regularized Movie Effect Model** - essentially we are re-using the movie-effect model from above with regularization. The general idea of penalized regression is to control the total variability of the movie effects; Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2$$

The first term of this equation is the least squares estimate we've been working with throughout; the second term is basically a penalty that gets larger when many $b_i$ are large. Using calculus, we can re-work this equation to show that the values of $b_i$ that minimize the equation are:
$$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$

where  $n_i$ is the number of ratings made for movie i. When the sample size is large, the penalty $\lambda$ is so low that it's basically ignored; however, when the sample size is small, the estimate $\hat{b}_i(\lambda)$ diminishes to 0. 

$\lambda$ is a tuning parameter, and we use cross-validation to choose the correct lambda for each case we decide to plug in regularization into. This, in essence, is what the function **find_generic_lambda** shown above is all about. 

```{r regularized_rmse_3, eval=FALSE}
```

The $\lambda$ values generated with this function, both the wide and narrowed ranges, can be seen on these plots: \newline

```{r echo=FALSE, include=TRUE, warning=FALSE, error=FALSE, fig.align='left'}
plot_lambdas(1, TRUE)
plot_lambdas(1, FALSE)
```

```{r  , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse3_lambda")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:rmse_results_output_3")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:rmse_3")
archivist::loadFromLocalRepo(hash)
```

The lowest-chosen $\lambda$ value is: **`r rmse3_lambda`** .\newline


The function **```regularized_rmse_3```**  was passed as a parameter into the **```find_generic_lambda```** function described above, and will produce the RMSE for the regularized movie effect model. It is of note that the result of this is only slightly better then the non-regularized Movie Effect model (model # 2) and significantly worse then the non-regularized model # 3. Moreover, an RMSE of **`r rmse_3`** is very high, and does not fit the criteria of being **< `r threshold_value`**. Therefor, the search for a fitting RMSE continues.

```{r  echo=FALSE}
rmse_results_output_3 %>%  knitr::kable()
```

\pagebreak

#### 4.2 Movie + User Bias Effect Regularized Model

The results from the regularization approach don't seem to be doing any better then the movie Effect model, and significantly worse then the user Effect + movie effect model. We will now try the regularization option with both movie and user bias effect. Therefor, I will next attempt to tune the model more with the both the **Movie Effect** AND the **User Effect** parameter.

This time, the formula we will follow is, with $b_u$ as the user Effect:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)$$ 

```{r regularized_movie_and_user, eval=FALSE}
```

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse4_lambda")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:tmp_rmse_results_4")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:rmse_4")
archivist::loadFromLocalRepo(hash)
```

\pagebreak

The  $\lambda$'s from this function can be seen here:\newline

```{r echo=FALSE, include=TRUE, warning=FALSE, error=FALSE, fig.align='left'}
plot_lambdas(2, TRUE)
plot_lambdas(2, FALSE)
```
The lowest-chosen $\lambda$ value is:  **`r rmse4_lambda`** .\newline
```{r  echo=FALSE}
tmp_rmse_results_4 %>%  knitr::kable()
```

It seems like the regularization model with both user and movie effects works very well with the RMSE, and even performed slightly better then the current winner,  **the user Effect + movie effect model**. But is an even better approach possible? I will next attempt to add the film's release year into the mix and examine whether this will help making the predictions more accurate.
\pagebreak

#### 4.3 Movie + User + Release Year Effect Regularized Model

But can we do even better? Next, I will attempt to add in the year into the mix, testing whether the age of the movie makes a difference.
For this we have created the field "age_of_movie"

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u - b_y \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2 + \sum_{i} b_y^2 \right)$$

with $b_y$ as the **release year effect**. 

```{r regularized_movie_and_user_and_year, eval=FALSE}
```

The  $\lambda$'s from this function can be seen here:\newline

```{r echo=FALSE, include=TRUE, warning=FALSE, error=FALSE, fig.align='left'}
plot_lambdas(3, TRUE)
plot_lambdas(3, FALSE)
```

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:model_5_lamdba")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:rmse_results_output_5")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:model_5_rmse")
archivist::loadFromLocalRepo(hash)
```

The lowest-chosen $\lambda$ value is:  **`r model_5_lamdba`** .\newline

```{r  echo=FALSE}
rmse_results_output_5 %>%  knitr::kable()
```

The added year does indeed seem to have made a difference in the calculations; The next item to be tested is the genre. 

#### 4.4 Movie + User + Release Year + Genre Effect Regularized Model

Following the pattern of our previous regularized models, we add in $b_g$ to represent the **genre effect** into the regularization mix, using the following formula:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u - b_y -b_g \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2 + \sum_{i} b_y^2 + \sum_{i} b_g^2 \right)$$

\pagebreak

```{r regularized_movie_and_user_and_year_and_genre, eval=FALSE}
```

\pagebreak

The  $\lambda$'s from this function can be seen here:\newline

```{r echo=FALSE, include=TRUE, warning=FALSE, error=FALSE, fig.align='left'}
plot_lambdas(4, TRUE)
plot_lambdas(4, FALSE)
```

```{r, echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:model_6_lambda")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:rmse_results_output_6")
archivist::loadFromLocalRepo(hash)
hash <- archivist::searchInLocalRepo("name:model_6_rmse")
archivist::loadFromLocalRepo(hash)
```

The lowest-chosen $\lambda$ value is: **`r model_6_lambda`**.\newline

```{r  echo=FALSE}
rmse_results_output_6 %>% knitr::kable()
```

\pagebreak
# Results

After creating a number of predictive algorithms, the best model as per the project requirements is the one with the least RMSE : **'Regularized Movie, User, Release Year and Genre Effect Model'** (This is model **#7** from the list enclosed below). Now that we've selected this model, it will be applied to the **validation data set** (which has been untouched as of yet) with the $\lambda$ value we calculated with the last function.

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:final_rmse")
archivist::loadFromLocalRepo(hash)
```

## The final RMSE value calculated from the validation set is: **`r final_rmse`**.

`r final_rmse` is well below the required threshold of **`r threshold_value`**.\newline\newline
**We can now review the complete set of RMSE calculations :**

```{r , echo=FALSE, include=FALSE, results='hide'}
hash <- archivist::searchInLocalRepo("name:rmse_results")
archivist::loadFromLocalRepo(hash)
```

```{r regularized_movie_and_user_and_year_and_genre_validation_results_final_ouput, echo=FALSE}
```

It is apparent from the numbers presented above that without regularization we cannot achieve RMSE numbers less then the threshold value of **`r threshold_value`**.\newline\newline 

I wanted to examine options for improvement of the algorithm and introduced regularization into the models. The simplest regularization model, using only the movie affect with an optimized lambda, was not good enough for our calculations, and actually regressed from our third model (shown above). However, regularization with movie AND user effect beat the previously best-performing model. Adding in the release year in model # 6 increased our performance even further, and finally, using regularization combined with the movie, user, release year and genre effects proved to be the most efficient model of the models tested.\newline\newline
**Applying this final model, #7, to the validation set, gave us a validated RMSE of `r final_rmse`**. 
\pagebreak

# Conclusion

## Report Summary

In this project, I have built a machine learning algorithm to predict movie ratings with MovieLens dataset, and estimate their accuracy with RMSE. The **regularized model including the effect of user, movie, release year and genre** is characterized by the lowest RMSE value of all I have examined, and is hence the optimal model to use for the present project - out of the models presented. 

The final rating model uses  **regularization**, which constrains the total variability of the effect sizes by penalizing large estimates that come from small sample sizes. With a tuning parameter $\lambda$ to generate a rating  $Y_{u,i}$ for movie $i$ by user $u$. These calculations can be summarized with the following formula:

$$\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u - b_y -b_g \right)^2 + \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2 + \sum_{i} b_y^2 + \sum_{i} b_g^2 \right)$$

where the variables are:

* **$\mu$** is the average rating across all movies
* **$b_i$** is the per-movie movie bias
* **$b_u$** is the per-user movie bias
* **$b_y$** is the age of movie (year) bias
* **$b_g$** is the genre bias

The first term in the equation above is the mean squared error calculation , and the second term is the penalty term that gets larger when all the effects listed above (movie effect, user effect, release year effect, and genre effect) are large. Re-working the equation using calculus we can show that we need a $\lambda$ value that will minimize the equation that adds the penalty for all of the effects mentioned. 

The **RMSE** arrived at with this model on the **validation dataset** is **`r final_rmse`** - which is sufficiently accurate, as it is lower than the initial evaluation criteria **`r threshold_value`** given by the goal of the present project.\newline\newline

**As mentioned above, due to the way the function ```createDataPartition``` works, movies with 3 reviews or less have been excluded from the dataset in order to prevent NA errors in the RMSE calculations.** 

\pagebreak

## Other Options

### Using Various Regression Functions.
During this process I have also attempted to run the predictions using the ```train()``` R function,   with methods that included Linear Regression ("lm"), LASSO regression ("glmnet"), and Generalized Linear Regression ("glm"). When conducted on a small part of the data set, these functions produced an inferior RMSE result, not in line with the threshold required. More importantly, all 3 of these methods crashed the system when attempted to be run on the full dataset - showing that these are not realistic tools to be used for datasets of this size (in the millions).

### Possible Improvements

I think improvements on the RMSE could be achieved by evaluating the project with other means. Different machine learning models could also improve the results further, but hardware limitations (memory and processing power) may be a constraint.

The possible evaluation models we can also attempt to get even better results would include:

* Matrix factorization / Single Value Decomposition (SVD) / Principal Component Analysis (PCA)
* Gradient Descent 
* SGD Code Walk


\pagebreak

# Appendix - Environment

```{r}
print("Operating System:")
version
```